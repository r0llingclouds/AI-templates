{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from firecrawl import FirecrawlApp\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Initialize the FirecrawlApp with your API key\n",
    "app = FirecrawlApp(api_key=os.getenv(\"FIREWCRAWL_API_KEY\"))\n",
    "\n",
    "# URLs to scrape, organized by category\n",
    "base_urls = [\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/base.html',\n",
    "]\n",
    "\n",
    "connection_urls = [\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/core_api.html',\n",
    "]\n",
    "\n",
    "foundation_models_urls = [\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_embeddings.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_model_inference.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_ts_model_inference.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_model.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_deployments.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/prompt_template_manager.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_helpers.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_text_extraction.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_schema.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_rerank.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/rate_limit.html',    \n",
    "]\n",
    "\n",
    "foundation_models_extensions_urls = [\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_langchain.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_llamaindex.html',\n",
    "    'https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_rag.html'    \n",
    "]\n",
    "\n",
    "# Combine all URLs into a single list\n",
    "all_urls = base_urls + connection_urls + foundation_models_urls + foundation_models_extensions_urls\n",
    "\n",
    "# Create directory structure\n",
    "def create_directories():\n",
    "    \"\"\"Create directory structure for organizing the scraped docs\"\"\"\n",
    "    os.makedirs(\"scraped_docs\", exist_ok=True)\n",
    "    os.makedirs(\"scraped_docs/base\", exist_ok=True)\n",
    "    os.makedirs(\"scraped_docs/connection\", exist_ok=True)\n",
    "    os.makedirs(\"scraped_docs/foundation_models\", exist_ok=True)\n",
    "    os.makedirs(\"scraped_docs/foundation_models_extensions\", exist_ok=True)\n",
    "    \n",
    "    print(\"Created directory structure.\")\n",
    "\n",
    "def get_filename_from_url(url):\n",
    "    \"\"\"Extract the filename from the URL\"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    path = parsed_url.path\n",
    "    filename = os.path.basename(path)\n",
    "    # Remove the .html extension and replace with .md\n",
    "    if filename.endswith('.html'):\n",
    "        filename = filename[:-5] + '.md'\n",
    "    return filename\n",
    "\n",
    "def determine_category(url):\n",
    "    \"\"\"Determine which category a URL belongs to\"\"\"\n",
    "    if url in base_urls:\n",
    "        return \"base\"\n",
    "    elif url in connection_urls:\n",
    "        return \"connection\"\n",
    "    elif url in foundation_models_extensions_urls:\n",
    "        return \"foundation_models_extensions\"\n",
    "    elif url in foundation_models_urls:\n",
    "        return \"foundation_models\"\n",
    "    else:\n",
    "        return \"misc\"  # Default category\n",
    "\n",
    "def scrape_url_with_retry(url, max_retries=3, delay=10):\n",
    "    \"\"\"Scrape a URL with retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Scraping {url} (Attempt {attempt + 1}/{max_retries})\")\n",
    "            scrape_status = app.scrape_url(\n",
    "                url,\n",
    "                params={'formats': ['markdown']}\n",
    "            )\n",
    "            \n",
    "            # Check if the scrape was successful\n",
    "            if scrape_status and 'markdown' in scrape_status:\n",
    "                return scrape_status\n",
    "            \n",
    "            print(f\"Scrape incomplete, retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {str(e)}\")\n",
    "            print(f\"Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(f\"Failed to scrape {url} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def save_markdown(markdown_content, category, filename):\n",
    "    \"\"\"Save the markdown content to a file\"\"\"\n",
    "    file_path = os.path.join(\"scraped_docs\", category, filename)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    \n",
    "    print(f\"Saved {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_metadata(url_results):\n",
    "    \"\"\"Save metadata about the scraping process\"\"\"\n",
    "    metadata = {\n",
    "        \"scrape_time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"total_urls\": len(all_urls),\n",
    "        \"successful_scrapes\": len([r for r in url_results if r[\"success\"]]),\n",
    "        \"failed_scrapes\": len([r for r in url_results if not r[\"success\"]]),\n",
    "        \"results\": url_results\n",
    "    }\n",
    "    \n",
    "    with open(\"scraped_docs/metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved metadata to scraped_docs/metadata.json\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to scrape all URLs\"\"\"\n",
    "    create_directories()\n",
    "    \n",
    "    # Display warning about responsible scraping\n",
    "    print(\"=\"*80)\n",
    "    print(\"RESPONSIBLE SCRAPING NOTICE:\")\n",
    "    print(\"This script includes delays between requests to be respectful to the server.\")\n",
    "    print(\"Please use this tool responsibly and in accordance with the website's terms of service.\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    url_results = []\n",
    "    \n",
    "    for url in all_urls:\n",
    "        category = determine_category(url)\n",
    "        filename = get_filename_from_url(url)\n",
    "        \n",
    "        print(f\"\\nProcessing {url} (Category: {category}, Filename: {filename})\")\n",
    "        \n",
    "        scrape_result = scrape_url_with_retry(url)\n",
    "        \n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"category\": category,\n",
    "            \"filename\": filename,\n",
    "            \"success\": False,\n",
    "            \"file_path\": None\n",
    "        }\n",
    "        \n",
    "        if scrape_result and 'markdown' in scrape_result:\n",
    "            markdown_content = scrape_result['markdown']\n",
    "            file_path = save_markdown(markdown_content, category, filename)\n",
    "            \n",
    "            result[\"success\"] = True\n",
    "            result[\"file_path\"] = file_path\n",
    "        \n",
    "        url_results.append(result)\n",
    "        \n",
    "        # Add a substantial delay to avoid hitting rate limits or triggering denial of service protections\n",
    "        delay_seconds = 5  # Increased delay to 5 seconds\n",
    "        print(f\"Waiting {delay_seconds} seconds before the next request...\")\n",
    "        time.sleep(delay_seconds)\n",
    "    \n",
    "    save_metadata(url_results)\n",
    "    \n",
    "    # Print summary\n",
    "    successful = len([r for r in url_results if r[\"success\"]])\n",
    "    print(f\"\\nScraping completed: {successful}/{len(all_urls)} URLs scraped successfully\")\n",
    "    \n",
    "    # Print failures if any\n",
    "    failures = [r for r in url_results if not r[\"success\"]]\n",
    "    if failures:\n",
    "        print(\"\\nFailed to scrape the following URLs:\")\n",
    "        for failure in failures:\n",
    "            print(f\"- {failure['url']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory structure.\n",
      "================================================================================\n",
      "RESPONSIBLE SCRAPING NOTICE:\n",
      "This script includes delays between requests to be respectful to the server.\n",
      "Please use this tool responsibly and in accordance with the website's terms of service.\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/base.html (Category: base, Filename: base.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/base.html (Attempt 1/3)\n",
      "Saved scraped_docs/base/base.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/core_api.html (Category: connection, Filename: core_api.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/core_api.html (Attempt 1/3)\n",
      "Saved scraped_docs/connection/core_api.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_embeddings.html (Category: foundation_models, Filename: fm_embeddings.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_embeddings.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_embeddings.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_model_inference.html (Category: foundation_models, Filename: fm_model_inference.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_model_inference.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_model_inference.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_ts_model_inference.html (Category: foundation_models, Filename: fm_ts_model_inference.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_ts_model_inference.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_ts_model_inference.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_model.html (Category: foundation_models, Filename: fm_model.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_model.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_model.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_deployments.html (Category: foundation_models, Filename: fm_deployments.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_deployments.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_deployments.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/prompt_template_manager.html (Category: foundation_models, Filename: prompt_template_manager.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/prompt_template_manager.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/prompt_template_manager.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_helpers.html (Category: foundation_models, Filename: fm_helpers.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_helpers.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_helpers.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_text_extraction.html (Category: foundation_models, Filename: fm_text_extraction.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_text_extraction.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_text_extraction.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_schema.html (Category: foundation_models, Filename: fm_schema.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_schema.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_schema.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_rerank.html (Category: foundation_models, Filename: fm_rerank.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_rerank.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/fm_rerank.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/rate_limit.html (Category: foundation_models, Filename: rate_limit.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/rate_limit.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models/rate_limit.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_langchain.html (Category: foundation_models_extensions, Filename: fm_extensions_langchain.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_langchain.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models_extensions/fm_extensions_langchain.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_llamaindex.html (Category: foundation_models_extensions, Filename: fm_extensions_llamaindex.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_llamaindex.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models_extensions/fm_extensions_llamaindex.md\n",
      "Waiting 5 seconds before the next request...\n",
      "\n",
      "Processing https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_rag.html (Category: foundation_models_extensions, Filename: fm_extensions_rag.md)\n",
      "Scraping https://ibm.github.io/watsonx-ai-python-sdk/fm_extensions_rag.html (Attempt 1/3)\n",
      "Saved scraped_docs/foundation_models_extensions/fm_extensions_rag.md\n",
      "Waiting 5 seconds before the next request...\n",
      "Saved metadata to scraped_docs/metadata.json\n",
      "\n",
      "Scraping completed: 16/16 URLs scraped successfully\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
