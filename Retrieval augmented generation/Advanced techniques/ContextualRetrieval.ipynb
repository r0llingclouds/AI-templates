{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach for retrieving documents from the index\n",
    "# Instead of vectorizing the documents, it uses the LLM to generate questions related to each chunk\n",
    "# These questions are then vectorized and used to search for the most similar chunks in the index\n",
    "# It can improve the accuracy of the retrieval, but it's more expensive and slower\n",
    "# Copy this snippet into your solution. You can find templates in other notebooks of this repo\n",
    "# You will need to first generate a list of chunks using any of the processors provided in the other notebooks of this repo\n",
    "# Then the function below will generate a new set of chunks, with each new chunk having a synthetic question related to the original chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick example. get some docs in a folder and run this\n",
    "import os\n",
    "path = 'docs'\n",
    "pdf_files = [os.path.join(path, filename) for filename in os.listdir(path) if filename.lower().endswith('.pdf')]\n",
    "\n",
    "from PDFChunker import PDFChunker\n",
    "processor = PDFChunker()\n",
    "chunks = processor.process_files(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "import os\n",
    "\n",
    "credentials = Credentials(\n",
    "    url = os.getenv(\"WATSONX_URL\"),\n",
    "    api_key = os.getenv(\"WATSONX_API\")\n",
    ")\n",
    "wx_client = APIClient(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding model that will create embeddings for the questions\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "\n",
    "embed_params = {\n",
    "    EmbedParams.TRUNCATE_INPUT_TOKENS: 512,\n",
    "    EmbedParams.RETURN_OPTIONS: {\n",
    "    'input_text': True\n",
    "    }\n",
    "}\n",
    "\n",
    "embedding_model = Embeddings(\n",
    "    model_id=wx_client.foundation_models.EmbeddingModels.MULTILINGUAL_E5_LARGE,\n",
    "    params=embed_params,\n",
    "    credentials=Credentials(\n",
    "        api_key=os.getenv(\"WATSONX_API\"),\n",
    "        url=os.getenv(\"WATSONX_URL\")\n",
    "    ),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the LLM that will generate questions for each chunk\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "contextual_retrieval_model = Model(\n",
    "            model_id=wx_client.foundation_models.TextModels.MISTRAL_LARGE,\n",
    "            params={GenParams.DECODING_METHOD: 'greedy',\n",
    "            GenParams.MIN_NEW_TOKENS: 1,\n",
    "            GenParams.MAX_NEW_TOKENS: 600,\n",
    "            GenParams.STOP_SEQUENCES: []\n",
    "            },\n",
    "            credentials={\n",
    "                \"apikey\": os.getenv(\"WATSONX_API\"),\n",
    "                \"url\": os.getenv(\"WATSONX_URL\"),\n",
    "            },\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextual retrieval: the function that will generate and embed example questions\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_and_embed_questions(\n",
    "    chunks: List[Dict[str, Any]],\n",
    "    llm_model: Any,\n",
    "    embedding_model: Any,\n",
    "    batch_size: int = 1\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate questions for each text chunk and create embeddings for them.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of dictionaries containing text chunks and their metadata\n",
    "        llm_model: Initialized LLM model for question generation\n",
    "        embedding_model: Initialized embedding model\n",
    "        batch_size: Number of chunks to process at once for batched operations\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with columns for chunk metadata, generated questions, and their embeddings\n",
    "    \"\"\"\n",
    "    # Template for question generation\n",
    "    prompt_contextual_questions = \"\"\"Genera un listado de 10 preguntas cortas para el siguiente fragmento de texto. Retornalas en formato JSON.\n",
    "\n",
    "Input: [INPUT] Condiciones Generales Súper Blindaje Santander Estimado Cliente, Agradecemos su confianza al haber contratado su Seguro de “Súper Blindaje Santander” con Zurich Santander Seguros México, S.A., para nosotros es un compromiso muy importante garantizar la satisfacción de sus necesidades de protección y prevención, brindando un servicio que cumpla y supere sus expectativas. Para respaldar este compromiso, Zurich Santander Seguros México, S. A., pone a su disposición una infraestructura de servicio a nivel nacional que cuenta con recursos tecnológicos y un equipo de profesionales para atenderle. Le recordamos revisar debidamente la Póliza y sus Condiciones Generales, en estos documentos encontrará los riesgos amparados, sumas aseguradas, el alcance de sus coberturas y qué hacer en caso de siniestro. Si tiene alguna duda o requiere información adicional, nuestros especialistas tendrán el gusto de atenderle y asesorarle en los teléfonos 5169 4300 en la Ciudad de México, o al 01 555169 4300 lada sin costo desde el interior del país. Atentamente, Zurich Santander Seguros México, S. A. [END_INPUT]\n",
    "Output: ```json\n",
    "{\n",
    "  \"preguntas\": [\n",
    "    \"¿Qué seguro ha contratado el cliente?\",\n",
    "    \"¿Qué empresa proporciona el seguro?\",\n",
    "    \"¿Qué compromiso tiene la empresa con el cliente?\",\n",
    "    \"¿Qué tipo de infraestructura ofrece la empresa?\",\n",
    "    \"¿Qué documentos debe revisar el cliente?\",\n",
    "    \"¿Qué información encontrará el cliente en la Póliza y sus Condiciones Generales?\",\n",
    "    \"¿Qué debe hacer el cliente en caso de siniestro?\",\n",
    "    \"¿Qué debe hacer el cliente si tiene dudas o necesita información adicional?\",\n",
    "    \"¿Cuál es el número de teléfono para contactar a los especialistas en la Ciudad de México?\",\n",
    "    \"¿Cuál es el número de teléfono para contactar a los especialistas desde el interior del país?\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Input: [INPUT] {input} [END_INPUT]\n",
    "Output:\"\"\"\n",
    "    \n",
    "    # Lists to store all results\n",
    "    all_records = []\n",
    "    \n",
    "    # Process chunks in batches\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Processing chunks\"):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        \n",
    "        # Generate questions for each chunk in the batch\n",
    "        for chunk in batch:\n",
    "            # Generate questions using the LLM\n",
    "            generated_text = llm_model.generate_text(\n",
    "                prompt=prompt_contextual_questions.replace('{input}', chunk['text'])\n",
    "            )\n",
    "\n",
    "            # for debugging\n",
    "            print(generated_text)\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            text_to_parse = generated_text.strip().replace('```json', '').replace('```', '')\n",
    "            questions_dict = json.loads(text_to_parse)\n",
    "            \n",
    "            # Get embeddings for all questions in this chunk\n",
    "            questions = questions_dict['preguntas']\n",
    "\n",
    "            question_embeddings = embedding_model.embed_documents(texts=[f\"query: {question}\" for question in questions])\n",
    "            \n",
    "            # Create a record for each question\n",
    "            for q_idx, (question, embedding) in enumerate(zip(questions, question_embeddings)):\n",
    "\n",
    "                record = {\n",
    "                    # Original chunk metadata\n",
    "                    'document_id': chunk['document_id'],\n",
    "                    'document_title': chunk['document_title'],\n",
    "                    'document_page': chunk['document_page'],\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'text': chunk['text'],\n",
    "                    'n_tokens': chunk['n_tokens'],\n",
    "                    \n",
    "                    # Question data\n",
    "                    'question_id': f\"{chunk['chunk_id']}_{q_idx}\",\n",
    "                    'question': question,\n",
    "                    'embedding': embedding\n",
    "                }\n",
    "                all_records.append(record)\n",
    "                    \n",
    "    return all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and embed questions\n",
    "enhanced_chunks = generate_and_embed_questions(\n",
    "    chunks=chunks,\n",
    "    llm_model=contextual_retrieval_model,\n",
    "    embedding_model=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the enhanced chunks to a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(enhanced_chunks)\n",
    "df.to_csv('enhanced_chunks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you want to load the enhanced chunks later on\n",
    "import pandas as pd\n",
    "df = pd.read_csv('enhanced_chunks.csv')\n",
    "enhanced_chunks = df.to_dict(orient='records')\n",
    "\n",
    "# parse embeddings stored in string format\n",
    "import ast\n",
    "for chunk in enhanced_chunks:\n",
    "    chunk['embedding'] = ast.literal_eval(chunk['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using elastic search\n",
    "\n",
    "# Settings for document processing. note it has more fields than a normal index due to the synthetic questions\n",
    "index_parameters = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'chunk_id': {'type': 'integer'},\n",
    "            'document_id': {'type': 'integer'},\n",
    "            'document_title': {'type': 'keyword'},\n",
    "            'document_page': {'type': 'integer'},\n",
    "            'n_tokens': {'type': 'integer'},\n",
    "            'text': {'type': 'text'},\n",
    "            'question': {'type': 'text'},\n",
    "            'question_id': {'type': 'keyword'},\n",
    "            'embedding': {\n",
    "                'dims': 1024,\n",
    "                'type': 'dense_vector',\n",
    "                'similarity': 'cosine',\n",
    "                'index': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Milvus\n",
    "fields = [\n",
    "            'chunk_id', 'document_id', 'document_title',\n",
    "            'document_page', 'n_tokens', 'text', 'question',\n",
    "            'question_id', 'embedding'\n",
    "        ]\n",
    "\n",
    "field_schema = [\n",
    "    FieldSchema(name=\"chunk_id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"document_id\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"document_title\", dtype=DataType.VARCHAR, max_length=255),\n",
    "    FieldSchema(name=\"document_page\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"n_tokens\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"question\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"question_id\", dtype=DataType.VARCHAR, max_length=255),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1024)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
