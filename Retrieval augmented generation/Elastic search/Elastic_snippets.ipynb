{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG template for watsonx discovery (Elasticsearch)\n",
    "# It provices a complete end-to-end example of how to use watsonx discovery to answer questions\n",
    "# Using watsonx.ai for foundation models and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "import os\n",
    "credentials = Credentials(\n",
    "    url = os.getenv(\"WATSONX_URL\"),\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    ")\n",
    "wx_client = APIClient(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load all pdf files\n",
    "path = 'docs'\n",
    "pdf_files = [os.path.join(path, filename) for filename in os.listdir(path) if filename.lower().endswith('.pdf')]\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing pdf files into chunks - option with PDF chunker\n",
    "# !pip install git+https://$PATIBM@github.ibm.com/tech-garage-spgi/pdf-chunker.git@16cc4c0d793bd11088999a1489e472e463a86077\n",
    "from PDFChunker import PDFChunker\n",
    "processor = PDFChunker()\n",
    "chunks = processor.process_files(pdf_files)\n",
    "\n",
    "# alternative options for processing pdf files into chunks\n",
    "\n",
    "# from FitzProcessor import FitzProcessor\n",
    "# processor = FitzProcessor()\n",
    "# chunks = processor.process_files(pdf_files)\n",
    "\n",
    "# from UnstructuredProcessor import UnstructuredProcessor\n",
    "# processor = UnstructuredProcessor()\n",
    "# chunks = processor.process_files(pdf_files)\n",
    "\n",
    "# processing markdown files into chunks with Markdown processor\n",
    "# path = 'markdown_output'\n",
    "# markdown_files = [os.path.join(path, filename) for filename in os.listdir(path) if filename.lower().endswith('.md')]\n",
    "# from MarkdownProcessor import MarkdownProcessor\n",
    "# processor = MarkdownProcessor(markdown_files, max_chunk_size=512, aggregate_chunks_flag=True)\n",
    "# chunks = processor.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing with docling\n",
    "# it will produce markdown files first\n",
    "# from docling.document_converter import DocumentConverter\n",
    "# it can even read webpages\n",
    "# source = \"https://github.com/DS4SD/docling\"\n",
    "# converter = DocumentConverter()\n",
    "# markdown_files = {} # dict of type filename: file content\n",
    "# for pdf_file in pdf_files:\n",
    "#     result = converter.convert(pdf_file)\n",
    "#     result_output = result.document.export_to_markdown()\n",
    "#     markdown_files[os.path.basename(pdf_file)] = result_output\n",
    "\n",
    "# to explore, chunking with docling\n",
    "# from docling_core.transforms.chunker import HierarchicalChunker\n",
    "# conv_res = DocumentConverter().convert(source)\n",
    "# doc = conv_res.document\n",
    "# chunks = list(HierarchicalChunker().chunk(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting chunks\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(chunks)\n",
    "df['n_tokens'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding model that will create embeddings for the questions\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "\n",
    "embed_params = {\n",
    "    EmbedParams.TRUNCATE_INPUT_TOKENS: 512,\n",
    "    EmbedParams.RETURN_OPTIONS: {\n",
    "    'input_text': True\n",
    "    }\n",
    "}\n",
    "\n",
    "embedding_model = Embeddings(\n",
    "    model_id=wx_client.foundation_models.EmbeddingModels.MULTILINGUAL_E5_LARGE,\n",
    "    params=embed_params,\n",
    "    credentials=Credentials(\n",
    "        api_key=os.getenv(\"WATSONX_API_KEY\"),\n",
    "        url=os.getenv(\"WATSONX_URL\")\n",
    "    ),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla embeddings\n",
    "# you can also use alternatives such as contextual retrieval\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "operations = []\n",
    "print(\"Generando embeddings\")\n",
    "start = time.time()\n",
    "for chunk in tqdm(chunks):\n",
    "    operations.append({\"index\": {\"_index\": index_name}})\n",
    "    text_to_vectorize = 'query: ' + chunk['text']\n",
    "    chunk['embedding'] = embedding_model.embed_documents(texts=[text_to_vectorize])[0] # can also use embed_query\n",
    "    # embedding_model.embed_query(text=description)\n",
    "    operations.append(chunk)\n",
    "end = time.time()\n",
    "execution_time = end - start\n",
    "print(f\"Generando embeddings: {execution_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to elasticsearch\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "elastic_client = Elasticsearch(\n",
    "        os.getenv(\"ELASTIC_URL\"),\n",
    "        basic_auth=(os.getenv(\"ELASTIC_USER\"), os.getenv(\"ELASTIC_PASSWORD\")),\n",
    "        verify_certs=False\n",
    "    )\n",
    "\n",
    "# Settings for document processing\n",
    "index_parameters = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'chunk_id': {'type': 'integer'},\n",
    "            'document_id': {'type': 'integer'},\n",
    "            'document_title': {'type': 'keyword'},\n",
    "            'document_page': {'type': 'integer'},\n",
    "            'n_tokens': {'type': 'integer'},\n",
    "            'text': {'type': 'text'},\n",
    "            'embedding': {\n",
    "                'dims': 1024,\n",
    "                'type': 'dense_vector',\n",
    "                'similarity': 'cosine',\n",
    "                'index': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete index if exists (warning!)\n",
    "elastic_client.indices.delete(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional list indices\n",
    "indices = [index for index in elastic_client.indices.get_alias(index=\"*\", allow_no_indices=True) if not index.startswith('.')]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index and populate\n",
    "elastic_client.indices.create(index=index_name, body=index_parameters)\n",
    "\n",
    "print(\"Indexando documentos\")\n",
    "start = time.time()\n",
    "elastic_client.bulk(index=index_name, operations=operations, refresh=True)\n",
    "end = time.time()\n",
    "print(f\"Indexación completada en {end - start:.2f} segundos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list documents in index\n",
    "def list_documents(index_name: str):\n",
    "    body = {\n",
    "            \"index\": index_name,\n",
    "            \"size\": 0,\n",
    "            \"aggs\": {\n",
    "                \"titles\": {\n",
    "                    \"composite\": {\n",
    "                        \"size\": 1000,\n",
    "                        \"sources\": [\n",
    "                            {\"document_title\": {\"terms\": {\"field\": \"document_title\"}}}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    response = elastic_client.search(**body)\n",
    "    files = [\n",
    "        bucket['key']['document_title']\n",
    "        for bucket in response.body['aggregations']['titles']['buckets']\n",
    "    ]\n",
    "    return files\n",
    "\n",
    "print(list_documents(index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to update the index with new documents\n",
    "if not elastic_client.indices.exists(index=index_name).body:\n",
    "    raise Exception(f\"El índice {index_name} no existe.\")\n",
    "\n",
    "files_in_es = list_documents(index_name)\n",
    "\n",
    "pdf_files_to_update = [\n",
    "    os.path.join(path, filename)\n",
    "    for filename in os.listdir(path)\n",
    "    if (filename.lower().endswith('.pdf') and os.path.basename(filename) not in files_in_es)\n",
    "]\n",
    "\n",
    "if len(pdf_files_to_update) > 0:\n",
    "    chunks = processor.process_files(pdf_files_to_update)\n",
    "\n",
    "    operations = []\n",
    "    print(\"Generando embeddings\")\n",
    "    start = time.time()\n",
    "    for chunk in tqdm(chunks):\n",
    "        operations.append({\"index\": {\"_index\": index_name}})\n",
    "        text_to_vectorize = 'query: ' + chunk['text']\n",
    "        chunk['embedding'] = embedding_model.embed_documents(texts=[text_to_vectorize])[0]\n",
    "        operations.append(chunk)\n",
    "    end = time.time()\n",
    "    execution_time = end - start\n",
    "    print(f\"Generando embeddings: {execution_time:.2f} segundos\")\n",
    "\n",
    "    print(\"Indexando documentos\")\n",
    "    start = time.time()\n",
    "    elastic_client.bulk(index=index_name, operations=operations, refresh=True)\n",
    "    end = time.time()\n",
    "    print(f\"Indexación completada en {end - start:.2f} segundos.\")\n",
    "else:\n",
    "    print(\"No hay nuevos documentos para actualizar el índice.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic search\n",
    "def search(question, index, embedding_model, search_params=None):\n",
    "    default_search_params = {\n",
    "            \"index\": index,\n",
    "            \"knn\": {\n",
    "                \"field\": \"embedding\",\n",
    "                \"query_vector\": embedding_model.embed_documents(texts=['query: ' + question])[0],\n",
    "                \"k\": 10,\n",
    "                \"num_candidates\": 100\n",
    "            }\n",
    "        }\n",
    "    search_params = {**default_search_params, **(search_params or {})}\n",
    "\n",
    "    response = elastic_client.search(**search_params)\n",
    "\n",
    "    if not response[\"hits\"][\"hits\"]:\n",
    "        return None, []\n",
    "\n",
    "    context = \"\\n\\n\\n\\n\".join(hit[\"_source\"][\"text\"] for hit in response[\"hits\"][\"hits\"])\n",
    "\n",
    "    return context, response['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"¿Cómo se hace la renovación automática de la póliza blindaje plus?\"\n",
    "context, search_results = search(question, index_name, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of hybrid search including reranking (reciprocal rank fusion)\n",
    "search_params = {\n",
    "    \"index\" : index_name,\n",
    "    \"query\": { \n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"match\": {\n",
    "                    \"text\": {\n",
    "                        \"query\": question,\n",
    "                        \"boost\": 1,\n",
    "                        \"fuzziness\": \"AUTO\"\n",
    "                        }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "   },\n",
    "    \"knn\": { \n",
    "        \"field\": \"embedding\",\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": 100,\n",
    "        \"query_vector\": embedding_model.embed_documents(texts=[question])[0]\n",
    "    },\n",
    "    \"rank\": {\n",
    "        \"rrf\": {\n",
    "            \"rank_constant\": 60,\n",
    "            \"window_size\":  100\n",
    "        }\n",
    "    },\n",
    "   \"size\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "credentials = Credentials(\n",
    "    url = os.getenv(\"WATSONX_URL\"),\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    ")\n",
    "wx_client = APIClient(credentials)\n",
    "\n",
    "# the LLM that will generate questions for each chunk\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "answering_model = ModelInference(\n",
    "            model_id=wx_client.foundation_models.TextModels.MISTRAL_LARGE,\n",
    "            params={GenParams.DECODING_METHOD: 'greedy',\n",
    "            GenParams.MIN_NEW_TOKENS: 1,\n",
    "            GenParams.MAX_NEW_TOKENS: 600,\n",
    "            GenParams.STOP_SEQUENCES: []\n",
    "            },\n",
    "            credentials={\n",
    "                \"apikey\": os.getenv(\"WATSONX_API_KEY\"),\n",
    "                \"url\": os.getenv(\"WATSONX_URL\"),\n",
    "            },\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION ANSWERING\n",
    "prompt_answer = \"\"\"Eres el asistente de la base de conocimiento. Contesta a la pregunta indicada abajo, utilizando parcial o totalmente los datos que se te proporcionan como contexto, sin información previa.\n",
    "\n",
    "    CONTEXTO: '''{}'''\n",
    "\n",
    "    PREGUNTA: {}\n",
    "\n",
    "    RESPUESTA: \"\"\"\n",
    "\n",
    "def answer_question(question: str, index_name: str, answering_model, embedding_model, prompt: str, streaming: bool = True, search_params=None, context=None, search_results=None):\n",
    "    \"\"\"Answers a single question using the knowledge base.\"\"\"\n",
    "\n",
    "    if context is None and search_results is None:\n",
    "        context, search_results = search(question, index_name, embedding_model, search_params)\n",
    "\n",
    "    if context is None:\n",
    "        return \"No existe contexto para responder a la pregunta.\", []\n",
    "\n",
    "    # # trim context to fit within max length\n",
    "    # from transformers import AutoTokenizer\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"fxmarty/tiny-llama-fast-tokenizer\")\n",
    "    # max_context_length = 2500\n",
    "\n",
    "    # available_tokens = max_context_length - sum(len(tokenizer.encode(text)) \n",
    "    #                                                 for text in [question, prompt])\n",
    "    # context = tokenizer.decode(\n",
    "    #     tokenizer.encode(context, max_length=available_tokens, truncation=True),\n",
    "    #     skip_special_tokens=True\n",
    "    # )\n",
    "\n",
    "    formatted_prompt = prompt.format(context, question)\n",
    "\n",
    "    if streaming:\n",
    "        answer = \"\"\n",
    "        for chunk in answering_model.generate_text_stream(formatted_prompt):\n",
    "            print(chunk, end='')\n",
    "            answer += chunk\n",
    "    else:\n",
    "        answer = answering_model.generate_text(formatted_prompt)\n",
    "\n",
    "    return answer, search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"¿Cómo se hace la renovación automática de la póliza blindaje plus?\"\n",
    "answer, search_results = answer_question(question, index_name, answering_model, embedding_model, prompt_answer, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Reranking\n",
    "# It reranks the chunks based on the question\n",
    "# After reranking, it keeps only the top chunks\n",
    "# It then uses these chunks to answer the question\n",
    "from ibm_watsonx_ai.foundation_models import Rerank\n",
    "from ibm_watsonx_ai.foundation_models.schema import RerankParameters\n",
    "\n",
    "rerank_params = RerankParameters(truncate_input_tokens = 512)\n",
    "wx_ranker = Rerank(\n",
    "    model_id=\"cross-encoder/ms-marco-minilm-l-12-v2\",\n",
    "    credentials=Credentials(\n",
    "        api_key = os.getenv(\"WATSONX_API_KEY\"),\n",
    "        url = os.getenv(\"WATSONX_URL\")\n",
    "    ),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")\n",
    "\n",
    "def reranking(query, search_results, rerank_params, top_percentage=0.5):\n",
    "    # top_percentage = 0.1 means only chunks at the top 10% of the interval [best-worse] will be considered\n",
    "\n",
    "    response = wx_ranker.generate(query=query, inputs=search_results, params=rerank_params)\n",
    "\n",
    "    best_score = response['results'][0]['score']\n",
    "    worst_score = response['results'][-1]['score']\n",
    "    threshold = best_score - top_percentage * (best_score - worst_score)\n",
    "\n",
    "    reranked_results = []\n",
    "    leftout_results = []\n",
    "    for result in response['results']:\n",
    "        if result['score'] > threshold:\n",
    "            reranked_results.append(search_results[result['index']])\n",
    "        else:\n",
    "            leftout_results.append(search_results[result['index']])\n",
    "\n",
    "    reranked_context = \"\\n\\n\\n\\n\".join(hit.text for hit in reranked_results)\n",
    "\n",
    "    return reranked_context, reranked_results, leftout_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of reranking\n",
    "question = \"¿Cómo se hace la renovación automática de la póliza blindaje plus?\"\n",
    "\n",
    "search_params = {\n",
    "        \"index\": index_name,\n",
    "        \"knn\": {\n",
    "            \"field\": \"embedding\",\n",
    "            \"query_vector\": embedding_model.embed_documents(texts=['query: ' + question])[0],\n",
    "            \"k\": 10,\n",
    "            \"num_candidates\": 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "question = \"¿Cómo se hace la renovación automática de la póliza blindaje plus?\"\n",
    "\n",
    "context, initial_search_results = search(question, index_name, embedding_model, search_params)\n",
    "reranked_context, reranked_results, leftout_results = reranking(question, initial_search_results, rerank_params, top_percentage=0.1)\n",
    "answer, search_results = answer_question(question, index_name, answering_model, embedding_model, prompt_answer, streaming=True, context=reranked_context, search_results=reranked_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Reflection\n",
    "# It evaluates the relevance of each chunk before answering\n",
    "# It selects only those chunks that are relevant to the question\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "credentials = Credentials(\n",
    "    url = os.getenv(\"WATSONX_URL\"),\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    ")\n",
    "client = APIClient(credentials)\n",
    "\n",
    "# the LLM that will generate questions for each chunk\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "reflection_model = ModelInference(\n",
    "            model_id=client.foundation_models.TextModels.MISTRAL_LARGE,\n",
    "            params={GenParams.DECODING_METHOD: 'greedy',\n",
    "            GenParams.MIN_NEW_TOKENS: 1,\n",
    "            GenParams.MAX_NEW_TOKENS: 20,\n",
    "            GenParams.STOP_SEQUENCES: ['Si', 'Sí', 'sí', 'si', 'No', 'no']\n",
    "            },\n",
    "            credentials={\n",
    "                \"apikey\": os.getenv(\"WATSONX_API_KEY\"),\n",
    "                \"url\": os.getenv(\"WATSONX_URL\"),\n",
    "            },\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection(question: str, search_results: list, reflection_model):\n",
    "    prompt_reflection = \"\"\"\n",
    "            Eres el asistente de la base de conocimiento. Contesta a la pregunta indicada abajo, en base a los datos que se te proporcionan como contexto. Se estricto y basate exclusivamente en la información proporcionada en el documento.\n",
    "\n",
    "            ¿La informacion dada en el contexto está relacionada con la pregunta? Explicalo, comienza la respuesta con un \"Si\" o un \"No\".\n",
    "\n",
    "            Contexto: '''{}'''\n",
    "\n",
    "            Pregunta: {}\n",
    "            \n",
    "            Respuesta: \"\"\"\n",
    "\n",
    "    selected_chunks = []\n",
    "    leftout_chunks = []\n",
    "\n",
    "    for hit in search_results:\n",
    "        relevance, _ = (lambda context: (\n",
    "        any(term in (answer := reflection_model.generate_text(prompt_reflection.format(context, question))).lower()\n",
    "            for term in [\"sí\", \"si\"]),\n",
    "        answer))(hit['_source']['text'])\n",
    "        (selected_chunks if relevance else leftout_chunks).append(hit)\n",
    "\n",
    "    selected_context = \"\\n\\n\\n\\n\".join(hit['_source']['text'] for hit in selected_chunks)\n",
    "    \n",
    "    return selected_context, selected_chunks, leftout_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of reflection\n",
    "question = \"¿Cómo se hace la renovación automática de la póliza blindaje plus?\"\n",
    "\n",
    "search_params = {\n",
    "        \"index\": index_name,\n",
    "        \"knn\": {\n",
    "            \"field\": \"embedding\",\n",
    "            \"query_vector\": embedding_model.embed_documents(texts=['query: ' + question])[0],\n",
    "            \"k\": 10,\n",
    "            \"num_candidates\": 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "question = \"¿Cómo se hace la renovación automática de la póliza blindaje plus?\"\n",
    "\n",
    "context, initial_search_results = search(search_params)\n",
    "selected_context, selected_search_results, leftout_search_results = reflection(question,\n",
    "                                                                                initial_search_results, \n",
    "                                                                                reflection_model)\n",
    "answer, search_results = answer_question(question, \n",
    "                                         index_name, \n",
    "                                         answering_model, \n",
    "                                         embedding_model, \n",
    "                                         prompt_answer, \n",
    "                                         streaming=True, \n",
    "                                         context=selected_context, \n",
    "                                         search_results=selected_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
