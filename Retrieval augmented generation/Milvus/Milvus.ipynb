{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG template for watsonx data (Milvus)\n",
    "# It provices a complete end-to-end example of how to use Milvus to answer questions\n",
    "# Using watsonx.ai for foundation models and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "import os\n",
    "credentials = Credentials(\n",
    "    url = os.getenv(\"WATSONX_URL\"),\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    ")\n",
    "wx_client = APIClient(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load all pdf files\n",
    "path = 'docs'\n",
    "pdf_files = [os.path.join(path, filename) for filename in os.listdir(path) if filename.lower().endswith('.pdf')]\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing pdf files into chunks - option with PDF chunker\n",
    "# !pip install git+https://$PATIBM@github.ibm.com/tech-garage-spgi/pdf-chunker.git@16cc4c0d793bd11088999a1489e472e463a86077\n",
    "from PDFChunker import PDFChunker\n",
    "processor = PDFChunker()\n",
    "chunks = processor.process_files(pdf_files)\n",
    "\n",
    "# alternative options for processing pdf files into chunks\n",
    "\n",
    "# from FitzProcessor import FitzProcessor\n",
    "# processor = FitzProcessor()\n",
    "# chunks = processor.process_files(pdf_files)\n",
    "\n",
    "# from UnstructuredProcessor import UnstructuredProcessor\n",
    "# processor = UnstructuredProcessor()\n",
    "# chunks = processor.process_files(pdf_files)\n",
    "\n",
    "# processing markdown files into chunks with Markdown processor\n",
    "# path = 'markdown_output'\n",
    "# markdown_files = [os.path.join(path, filename) for filename in os.listdir(path) if filename.lower().endswith('.md')]\n",
    "# from MarkdownProcessor import MarkdownProcessor\n",
    "# processor = MarkdownProcessor(markdown_files, max_chunk_size=512, aggregate_chunks_flag=True)\n",
    "# chunks = processor.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing with docling\n",
    "# it will produce markdown files first\n",
    "# from docling.document_converter import DocumentConverter\n",
    "# it can even read webpages\n",
    "# source = \"https://github.com/DS4SD/docling\"\n",
    "# converter = DocumentConverter()\n",
    "# markdown_files = {} # dict of type filename: file content\n",
    "# for pdf_file in pdf_files:\n",
    "#     result = converter.convert(pdf_file)\n",
    "#     result_output = result.document.export_to_markdown()\n",
    "#     markdown_files[os.path.basename(pdf_file)] = result_output\n",
    "\n",
    "# to explore, chunking with docling\n",
    "# from docling_core.transforms.chunker import HierarchicalChunker\n",
    "# conv_res = DocumentConverter().convert(source)\n",
    "# doc = conv_res.document\n",
    "# chunks = list(HierarchicalChunker().chunk(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resulting chunks\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(chunks)\n",
    "df['n_tokens'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding model that will create embeddings for the questions\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "\n",
    "embed_params = {\n",
    "    EmbedParams.TRUNCATE_INPUT_TOKENS: 512,\n",
    "    EmbedParams.RETURN_OPTIONS: {\n",
    "    'input_text': True\n",
    "    }\n",
    "}\n",
    "\n",
    "embedding_model = Embeddings(\n",
    "    model_id=wx_client.foundation_models.EmbeddingModels.MULTILINGUAL_E5_LARGE,\n",
    "    params=embed_params,\n",
    "    credentials=Credentials(\n",
    "        api_key=os.getenv(\"WATSONX_API_KEY\"),\n",
    "        url=os.getenv(\"WATSONX_URL\")\n",
    "    ),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla embeddings\n",
    "# you can also use alternatives such as contextual retrieval\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Generating embeddings\")\n",
    "start = time.time()\n",
    "for chunk in tqdm(chunks):\n",
    "    text_to_vectorize = 'query: ' + chunk['text']\n",
    "    chunk['embedding'] = embedding_model.embed_documents(texts=[text_to_vectorize])[0]\n",
    "end = time.time()\n",
    "execution_time = end - start\n",
    "print(f\"Generating embeddings: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"test_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")\n",
    "milvus_client = connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list collections\n",
    "from pymilvus import utility\n",
    "\n",
    "utility.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete collection. warning!\n",
    "# if utility.has_collection(index_name):\n",
    "#     utility.drop_collection(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty collection\n",
    "from pymilvus import Collection, FieldSchema, DataType, CollectionSchema\n",
    "\n",
    "fields = [\n",
    "            'chunk_id', 'document_id', 'document_title',\n",
    "            'document_page', 'n_tokens', 'text',\n",
    "            'embedding'\n",
    "        ]\n",
    "\n",
    "index_params = {\n",
    "            \"metric_type\": \"COSINE\",\n",
    "            \"index_type\": \"FLAT\",\n",
    "            \"params\": {\n",
    "                \"nlist\": 1024\n",
    "            }\n",
    "        }\n",
    "\n",
    "field_schema = [\n",
    "    FieldSchema(name=\"chunk_id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"document_id\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"document_title\", dtype=DataType.VARCHAR, max_length=255),\n",
    "    FieldSchema(name=\"document_page\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"n_tokens\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1024)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = CollectionSchema(fields=field_schema)\n",
    "collection = Collection(name=index_name, schema=schema)\n",
    "collection.create_index(field_name='embedding', index_params=index_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate collection\n",
    "print(\"Indexing documents\")\n",
    "start = time.time()\n",
    "collection = Collection(index_name)\n",
    "collection.insert(chunks)\n",
    "collection.flush()\n",
    "collection.load()\n",
    "end = time.time()\n",
    "execution_time = end - start\n",
    "print(f\"Indexing documents: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get documents in collection\n",
    "from pymilvus import Collection\n",
    "collection = Collection(index_name)\n",
    "collection.load()\n",
    "\n",
    "def list_documents(index_name: str):\n",
    "    results = Collection(index_name).query(expr=\"document_title != ''\", output_fields=[\"document_title\"], limit=None)\n",
    "    files = [result[\"document_title\"] for result in results]\n",
    "    return list(set(files))\n",
    "\n",
    "list_documents(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to update the index with new documents\n",
    "if not utility.has_collection(index_name):\n",
    "    raise Exception(f\"El Ã­ndice {index_name} no existe.\")\n",
    "\n",
    "files_in_milvus = list_documents(index_name)\n",
    "\n",
    "pdf_files_to_update = [\n",
    "    os.path.join(path, filename)\n",
    "    for filename in os.listdir(path)\n",
    "    if (filename.lower().endswith('.pdf') and os.path.basename(filename) not in files_in_milvus)\n",
    "]\n",
    "\n",
    "if len(pdf_files_to_update) > 0:\n",
    "    chunks = processor.process_files(pdf_files_to_update)\n",
    "\n",
    "    print(\"Generating embeddings\")\n",
    "    start = time.time()\n",
    "    for chunk in tqdm(chunks):\n",
    "        text_to_vectorize = 'query: ' + chunk['text']\n",
    "        chunk['embedding'] = embedding_model.embed_documents(texts=[text_to_vectorize])[0]\n",
    "    end = time.time()\n",
    "    execution_time = end - start\n",
    "    print(f\"Generating embeddings: {execution_time:.2f} seconds\")\n",
    "\n",
    "    # populate collection\n",
    "    print(\"Indexing documents\")\n",
    "    start = time.time()\n",
    "    collection = Collection(index_name)\n",
    "    collection.insert(chunks)\n",
    "    collection.flush()\n",
    "    collection.load()\n",
    "    end = time.time()\n",
    "    execution_time = end - start\n",
    "    print(f\"Indexing documents: {execution_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"No hay nuevos documentos para actualizar el Ã­ndice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic search\n",
    "def search(question, index, embedding_model, search_params=None):\n",
    "    collection = Collection(name=index)\n",
    "    collection.load()\n",
    "\n",
    "    default_search_params = {\n",
    "            \"data\": [embedding_model.embed_documents(texts=['query: ' + question])[0]],\n",
    "            \"anns_field\": 'embedding',\n",
    "            \"param\": {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "            \"limit\": 4,\n",
    "            \"output_fields\": fields\n",
    "            #\"expr\": f\"document_title == '{doc_title}'\"\n",
    "        }\n",
    "    search_params = {**default_search_params, **(search_params or {})}\n",
    "\n",
    "    response = collection.search(**search_params)[0]\n",
    "\n",
    "    if len(response) == 0:\n",
    "        return None, []\n",
    "\n",
    "    context = \"\\n\\n\\n\\n\".join(hit.text for hit in response)\n",
    "\n",
    "    return context, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Â¿CÃ³mo se hace la renovaciÃ³n automÃ¡tica de la pÃ³liza blindaje plus?\"\n",
    "context, search_results = search(question, index_name, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "credentials = Credentials(\n",
    "    url = os.getenv(\"WATSONX_URL\"),\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    ")\n",
    "wx_client = APIClient(credentials)\n",
    "\n",
    "# the LLM that will generate questions for each chunk\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "answering_model = ModelInference(\n",
    "            model_id=wx_client.foundation_models.TextModels.MISTRAL_LARGE,\n",
    "            params={GenParams.DECODING_METHOD: 'greedy',\n",
    "            GenParams.MIN_NEW_TOKENS: 1,\n",
    "            GenParams.MAX_NEW_TOKENS: 600,\n",
    "            GenParams.STOP_SEQUENCES: []\n",
    "            },\n",
    "            credentials={\n",
    "                \"apikey\": os.getenv(\"WATSONX_API_KEY\"),\n",
    "                \"url\": os.getenv(\"WATSONX_URL\"),\n",
    "            },\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION ANSWERING\n",
    "prompt_answer = \"\"\"Eres el asistente de la base de conocimiento. Contesta a la pregunta indicada abajo, utilizando parcial o totalmente los datos que se te proporcionan como contexto, sin informaciÃ³n previa.\n",
    "\n",
    "    CONTEXTO: '''{}'''\n",
    "\n",
    "    PREGUNTA: {}\n",
    "\n",
    "    RESPUESTA: \"\"\"\n",
    "\n",
    "def answer_question(question: str, index_name: str, answering_model, embedding_model, prompt: str, streaming: bool = True, search_params=None, context=None, search_results=None):\n",
    "    \"\"\"Answers a single question using the knowledge base.\"\"\"\n",
    "\n",
    "    if context is None and search_results is None:\n",
    "        context, search_results = search(question, index_name, embedding_model, search_params)\n",
    "\n",
    "    if context is None:\n",
    "        return \"No existe contexto para responder a la pregunta.\", []\n",
    "\n",
    "    # # trim context to fit within max length\n",
    "    # from transformers import AutoTokenizer\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"fxmarty/tiny-llama-fast-tokenizer\")\n",
    "    # max_context_length = 2500\n",
    "\n",
    "    # available_tokens = max_context_length - sum(len(tokenizer.encode(text)) \n",
    "    #                                                 for text in [question, prompt])\n",
    "    # context = tokenizer.decode(\n",
    "    #     tokenizer.encode(context, max_length=available_tokens, truncation=True),\n",
    "    #     skip_special_tokens=True\n",
    "    # )\n",
    "\n",
    "    formatted_prompt = prompt.format(context, question)\n",
    "\n",
    "    if streaming:\n",
    "        answer = \"\"\n",
    "        for chunk in answering_model.generate_text_stream(formatted_prompt):\n",
    "            print(chunk, end='')\n",
    "            answer += chunk\n",
    "    else:\n",
    "        answer = answering_model.generate_text(formatted_prompt)\n",
    "\n",
    "    return answer, search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Â¿CÃ³mo se hace la renovaciÃ³n automÃ¡tica de la pÃ³liza blindaje plus?\"\n",
    "answer, search_results = answer_question(question, index_name, answering_model, embedding_model, prompt_answer, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Reranking\n",
    "# It reranks the chunks based on the question\n",
    "# After reranking, it keeps only the top chunks\n",
    "# It then uses these chunks to answer the question\n",
    "from ibm_watsonx_ai.foundation_models import Rerank\n",
    "from ibm_watsonx_ai.foundation_models.schema import RerankParameters\n",
    "\n",
    "rerank_params = RerankParameters(truncate_input_tokens = 512)\n",
    "wx_ranker = Rerank(\n",
    "    model_id=\"cross-encoder/ms-marco-minilm-l-12-v2\",\n",
    "    credentials=Credentials(\n",
    "        api_key = os.getenv(\"WATSONX_API_KEY\"),\n",
    "        url = os.getenv(\"WATSONX_URL\")\n",
    "    ),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")\n",
    "\n",
    "def reranking(query, search_results, rerank_params, top_percentage=0.5):\n",
    "    # top_percentage = 0.1 means only chunks at the top 10% of the interval [best-worse] will be considered\n",
    "\n",
    "    response = wx_ranker.generate(query=query, inputs=search_results, params=rerank_params)\n",
    "\n",
    "    best_score = response['results'][0]['score']\n",
    "    worst_score = response['results'][-1]['score']\n",
    "    threshold = best_score - top_percentage * (best_score - worst_score)\n",
    "\n",
    "    reranked_results = []\n",
    "    leftout_results = []\n",
    "    for result in response['results']:\n",
    "        if result['score'] > threshold:\n",
    "            reranked_results.append(search_results[result['index']])\n",
    "        else:\n",
    "            leftout_results.append(search_results[result['index']])\n",
    "\n",
    "    reranked_context = \"\\n\\n\".join(hit.text for hit in reranked_results)\n",
    "\n",
    "    return reranked_context, reranked_results, leftout_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of reranking\n",
    "question = \"Â¿CÃ³mo se hace la renovaciÃ³n automÃ¡tica de la pÃ³liza blindaje plus?\"\n",
    "\n",
    "search_params = {\n",
    "            \"data\": [embedding_model.embed_documents(texts=['query: ' + question])[0]],\n",
    "            \"anns_field\": 'embedding',\n",
    "            \"param\": {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "            \"limit\": 10,\n",
    "            \"output_fields\": fields\n",
    "        }\n",
    "\n",
    "question = \"Â¿CÃ³mo se hace la renovaciÃ³n automÃ¡tica de la pÃ³liza blindaje plus?\"\n",
    "\n",
    "context, initial_search_results = search(question, index_name, embedding_model)\n",
    "reranked_context, reranked_results, leftout_results = reranking(question, initial_search_results, rerank_params, top_percentage=0.1)\n",
    "answer, search_results = answer_question(question, index_name, answering_model, embedding_model, prompt_answer, streaming=True, context=reranked_context, search_results=reranked_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Reflection\n",
    "# It evaluates the relevance of each chunk before answering\n",
    "# It selects only those chunks that are relevant to the question\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "credentials = Credentials(\n",
    "    url = os.getenv(\"WATSONX_URL\"),\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    ")\n",
    "client = APIClient(credentials)\n",
    "\n",
    "# the LLM that will generate questions for each chunk\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "reflection_model = ModelInference(\n",
    "            model_id=client.foundation_models.TextModels.MISTRAL_LARGE, \n",
    "            params={GenParams.DECODING_METHOD: 'greedy',\n",
    "            GenParams.MIN_NEW_TOKENS: 1,\n",
    "            GenParams.MAX_NEW_TOKENS: 300,\n",
    "            GenParams.STOP_SEQUENCES: ['Si', 'SÃ­', 'sÃ­', 'si', 'No', 'no']\n",
    "            },\n",
    "            credentials={\n",
    "                \"apikey\": os.getenv(\"WATSONX_API_KEY\"),\n",
    "                \"url\": os.getenv(\"WATSONX_URL\"),\n",
    "            },\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection(question: str, search_results: list, reflection_model):\n",
    "    prompt_reflection = \"\"\"\n",
    "            Eres el asistente de la base de conocimiento. Contesta a la pregunta indicada abajo, en base a los datos que se te proporcionan como contexto. Se estricto y basate exclusivamente en la informaciÃ³n proporcionada en el documento.\n",
    "\n",
    "            Â¿La informacion dada en el contexto estÃ¡ relacionada con la pregunta? Explicalo, comienza la respuesta con un \"Si\" o un \"No\".\n",
    "\n",
    "            Contexto: '''{}'''\n",
    "\n",
    "            Pregunta: {}\n",
    "            \n",
    "            Respuesta: \"\"\"\n",
    "\n",
    "    selected_chunks = []\n",
    "    leftout_chunks = []\n",
    "\n",
    "    for hit in search_results:\n",
    "        relevance, _ = (lambda context: (\n",
    "        any(term in (answer := reflection_model.generate_text(prompt_reflection.format(context, question))).lower()\n",
    "            for term in [\"sÃ­\", \"si\"]),\n",
    "        answer))(hit.text)\n",
    "        (selected_chunks if relevance else leftout_chunks).append(hit)\n",
    "\n",
    "    selected_context = \"\\n\\n\\n\\n\".join(hit.text for hit in selected_chunks)\n",
    "    \n",
    "    return selected_context, selected_chunks, leftout_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of reflection\n",
    "question = \"Â¿CÃ³mo se hace la renovaciÃ³n automÃ¡tica de la pÃ³liza blindaje plus?\"\n",
    "\n",
    "search_params = {\n",
    "            \"data\": [embedding_model.embed_documents(texts=['query: ' + question])[0]],\n",
    "            \"anns_field\": 'embedding',\n",
    "            \"param\": {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "            \"limit\": 10,\n",
    "            \"output_fields\": fields\n",
    "        }\n",
    "\n",
    "question = \"Â¿CÃ³mo se hace la renovaciÃ³n automÃ¡tica de la pÃ³liza blindaje plus?\"\n",
    "\n",
    "context, initial_search_results = search(search_params)\n",
    "selected_context, selected_search_results, leftout_search_results = reflection(question,\n",
    "                                                                                initial_search_results, \n",
    "                                                                                reflection_model)\n",
    "answer, search_results = answer_question(question, \n",
    "                                         index_name, \n",
    "                                         answering_model, \n",
    "                                         embedding_model, \n",
    "                                         prompt_answer, \n",
    "                                         streaming=True, \n",
    "                                         context=selected_context, \n",
    "                                         search_results=selected_search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
